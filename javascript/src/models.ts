// ====================
// Language Model Types
// ====================

export interface TextPart {
  type: "text";
  text: string;
}

export interface ImagePart {
  type: "image";
  mimeType: string;
  imageData: string;
}

export interface ToolCallPart {
  type: "tool-call";
  /**
   * The ID of the tool call, used to match the tool result with the tool call.
   */
  toolCallId: string;
  toolName: string;
  args: unknown;
}

export interface ToolResultPart {
  type: "tool-result";
  /**
   * The ID of the tool call from previous assistant message.
   */
  toolCallId: string;
  toolName: string;
  result: unknown;
  /**
   * Marks the tool result as an error.
   */
  isError?: boolean;
}

/**
 * Represents a message sent by the user.
 */
export interface UserMessage {
  role: "user";
  content: (TextPart | ImagePart)[];
}

/**
 * Represents a message generated by the model.
 */
export interface AssistantMessage {
  role: "assistant";
  content: (TextPart | ToolCallPart)[];
}

export interface PartialAssistantMessage {
  index: number;
  part: TextPart | ToolCallPart;
}

/**
 * Represents a tool that can be used by the model.
 */
export interface Tool {
  name: string;
  description: string;
  parameters: unknown;
}

/**
 * Represents tool result in the message history.
 */
export interface ToolMessage {
  role: "tool";
  content: ToolResultPart[];
}

export type Message = UserMessage | AssistantMessage | ToolMessage;

export interface ModelUsage {
  inputTokens: number;
  outputTokens: number;
}

export interface ModelResponse {
  message: AssistantMessage;
  usage?: ModelUsage;
}

export interface PartialModelResponse {
  delta: PartialAssistantMessage;
}

// ====================
// Language Model API Abstraction
// ====================

export interface LanguageModel {
  /**
   * The provider of the model, e.g. "openai", "anthropic", "google"
   */
  provider: string;
  /**
   * The ID of the model, e.g. "gpt-3.5-turbo", "haiku"
   */
  modelId: string;
  /**
   * Generates a response to the given input.
   */
  generate(input: LanguageModelInput): Promise<ModelResponse>;
  /**
   * Generates a response to the given input, returning a stream of partial responses.
   */
  stream(
    input: LanguageModelInput,
  ): AsyncGenerator<PartialModelResponse, ModelResponse>;
}

export interface LanguageModelInput {
  /**
   * A system prompt is a way of providing context and instructions to the model
   */
  systemPrompt?: string;
  /**
   * A list of messages comprising the conversation so far.
   */
  messages: Message[];
  /**
   * Definitions of tools that the model may use.
   */
  tools?: Tool[];
  /**
   * Determines how the model should choose which tool to use.
   * "auto" - The model will automatically choose the tool to use or not use any tools.
   * "none" - The model will not use any tools.
   * "required" - The model will be forced to use a tool.
   * { type: "tool", toolName: "toolName" } - The model will use the specified tool.
   */
  toolChoice?:
    | { type: "auto" }
    | { type: "none" }
    | { type: "required" }
    | { type: "tool"; toolName: string };
  /**
   * The maximum number of tokens that can be generated in the chat completion.
   */
  maxTokens?: number;
  /**
   * Amount of randomness injected into the response.
   * Ranges from 0.0 to 1.0
   */
  temperature?: number;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass
   * Ranges from 0.0 to 1.0
   */
  topP?: number;
  /**
   * Only sample from the top K options for each subsequent token. Used to remove "long tail" low probability responses.
   * Ranges from 0.0 to 1.0
   */
  topK?: number;
  /**
   * Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
   * @todo: standardize the range of this parameter
   */
  presencePenalty?: number;
  /**
   * Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
   * @todo: standardize the range of this parameter
   */
  frequencyPenalty?: number;
  /**
   * The seed (integer), if set and supported by the model, to enable deterministic results.
   */
  seed?: number;
}
